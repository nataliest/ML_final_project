{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# import tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test by recording sound [y/n]: n\n"
     ]
    }
   ],
   "source": [
    "import pyaudio # source ~./bash... unset PYTHONPATH\n",
    "import wave\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import librosa.feature\n",
    "\n",
    "do_rec = input(\"Test by recording sound [y/n]: \")\n",
    "\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 1\n",
    "RATE = 8000\n",
    "CHUNK = 1024\n",
    "RECORD_SECONDS = 5\n",
    "WAVE_OUTPUT_FILENAME = \"test_audio.wav\"\n",
    "IMG_EXT = \".png\"\n",
    "\n",
    "if do_rec.lower() == 'y':\n",
    "    audio = pyaudio.PyAudio()\n",
    "\n",
    "    # start Recording\n",
    "    stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "                    rate=RATE, input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "    print (\"recording...\")\n",
    "    frames = []\n",
    "\n",
    "    for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "    print (\"finished recording\")\n",
    "\n",
    "\n",
    "    # stop Recording\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "\n",
    "    waveFile = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "    waveFile.setnchannels(CHANNELS)\n",
    "    waveFile.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "    waveFile.setframerate(RATE)\n",
    "    waveFile.writeframes(b''.join(frames))\n",
    "    waveFile.close()\n",
    "    \n",
    "#     plt.figure(figsize=(12, 12))\n",
    "    y, sr = librosa.load(WAVE_OUTPUT_FILENAME)\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=256, fmax=4000)\n",
    "    librosa.display.specshow(librosa.logamplitude(S,ref_power=np.max),\n",
    "                            fmax=4000)\n",
    "    output_filename = WAVE_OUTPUT_FILENAME[:-4] + \"_spec\" + IMG_EXT\n",
    "    plt.savefig(output_filename, bbox_inches='tight', pad_inches = 0)\n",
    "    librosa.display.specshow(librosa.logamplitude(S,ref_power=np.max),\n",
    "                             y_axis='mel', fmax=4000, x_axis='time')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Mel spectrogram')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Convert audio files to spectrograms:\\n\\n\\n\\ndirectory = \"../test_audio/\"\\n\\nplt.figure(figsize=(12, 12))\\nfor filename in os.listdir(directory):\\n\\n    if filename[:1] != \\'.\\':\\n        path = os.path.join(directory, filename)\\n\\n        y, sr = librosa.load(path)\\n        S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=256, fmax=4000)\\n        librosa.display.specshow(librosa.logamplitude(S,ref_power=np.max),\\n                                fmax=4000)\\n        output_filename = \"../test_spectrograms/\" + filename[:-4] + \"_spec\" + IMG_EXT\\n        plt.savefig(output_filename, bbox_inches=\\'tight\\', pad_inches = 0)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Convert audio files to spectrograms:\n",
    "\n",
    "\n",
    "\n",
    "directory = \"../test_audio/\"\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "for filename in os.listdir(directory):\n",
    "\n",
    "    if filename[:1] != '.':\n",
    "        path = os.path.join(directory, filename)\n",
    "\n",
    "        y, sr = librosa.load(path)\n",
    "        S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=256, fmax=4000)\n",
    "        librosa.display.specshow(librosa.logamplitude(S,ref_power=np.max),\n",
    "                                fmax=4000)\n",
    "        output_filename = \"../test_spectrograms/\" + filename[:-4] + \"_spec\" + IMG_EXT\n",
    "        plt.savefig(output_filename, bbox_inches='tight', pad_inches = 0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# import PIL\n",
    "# from PIL import Image\n",
    "\n",
    "# def grayscale(picture):\n",
    "#     res=PIL.Image.new(picture.mode, picture.size)\n",
    "#     width, height = picture.size\n",
    "\n",
    "#     for i in range(0, width):\n",
    "#         for j in range(0, height):\n",
    "#             pixel=picture.getpixel((i,j))\n",
    "#             avg=int((pixel[0]+pixel[1]+pixel[2])/3)\n",
    "#             res.putpixel((i,j),(avg,avg,avg))\n",
    "#     res.show()\n",
    "\n",
    "# image_fp = \"../test_spectrograms/sample19_spec.png\"\n",
    "# im = Image.open(image_fp)\n",
    "# grayscale(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "\n",
    "import PIL\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Returns numpy image at size imageSize*imageSize\n",
    "def getProcessedData(img,imageSize, grayscale):\n",
    "    numchannels = 4\n",
    "    if grayscale:\n",
    "        numchannels = 2\n",
    "\n",
    "        \n",
    "                \n",
    "    img = img.resize((imageSize,imageSize), resample=Image.ANTIALIAS)\n",
    "#     print(np.shape(img))\n",
    "#     print(128*128*4)\n",
    "# #     imgData = np.asarray(img, dtype=np.uint8).reshape(imageSize,imageSize,1)\n",
    "#     imgData = np.asarray(img, dtype=np.uint8)\n",
    "#     print(np.shape(imgData))\n",
    "    imgData = np.array(img, dtype=np.uint8).reshape(img.size[0], img.size[1], numchannels)\n",
    "#     print(imgData)\n",
    "\n",
    "    imgData = imgData[:, :, :-1]\n",
    "    imgData = imgData/255.\n",
    "#     print(np.shape(imgData))\n",
    "#     print(imgData)\n",
    "#     img.save('greyscale.png')\n",
    "    return imgData\n",
    "\n",
    "#Returns numpy image at size imageSize*imageSize\n",
    "def getImageData(filename,imageSize, grayscale):\n",
    "    if grayscale:\n",
    "        img = Image.open(filename).convert('LA')\n",
    "    else:\n",
    "        img = Image.open(filename)\n",
    "\n",
    "#     height, width, channels = scipy.ndimage.imread(filename).shape\n",
    "#     print(\"&&\")\n",
    "#     print(height, width, channels)\n",
    "#     print(\"&&\")\n",
    "    imgData = getProcessedData(img, imageSize, grayscale)\n",
    "    return imgData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import pickle\n",
    "\n",
    "directory = \"../test_spectrograms/\"\n",
    "\n",
    "def createDataset(size, validationRatio, testRatio, grayscale):\n",
    "    data = []\n",
    "    numchannels = 3\n",
    "    if grayscale:\n",
    "        numchannels = 1\n",
    "    count = 1\n",
    "    #Add data (X,y)\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename[:1] != '.':\n",
    "#             if count > 10:\n",
    "#                 break\n",
    "            path = os.path.join(directory, filename)\n",
    "            imgData = getImageData(path, size, grayscale)\n",
    "            label = 1\n",
    "            if filename[:4] == \"nois\" or filename[:4] == \"audi\":\n",
    "                label = 0\n",
    "            data.append((imgData,label))\n",
    "#             count += 1\n",
    "    print(\"data size\", np.shape(data))\n",
    "#     print(data)\n",
    "    #Shuffle data\n",
    "    shuffle(data)\n",
    "#     print(data)\n",
    "    print(\"data size\", np.shape(data))\n",
    "    #Extract X and y\n",
    "    X,y = zip(*data)\n",
    "    print(\"X size\", np.shape(X))\n",
    "    print(\"y size\", np.shape(y))\n",
    "    #Split data\n",
    "    validationNb = int(len(X)*validationRatio)\n",
    "    testNb = int(len(X)*testRatio)\n",
    "    trainNb = len(X)-(validationNb + testNb)\n",
    "    print(validationNb, testNb, trainNb)\n",
    "\n",
    "    #Prepare for Tflearn at the same time\n",
    "    train_X = np.array(X[:trainNb]).reshape([-1, size, size, 1])\n",
    "    train_y = np.array(y[:trainNb])\n",
    "    validation_X = np.array(X[trainNb:trainNb+validationNb]).reshape([-1, size, size, 1])\n",
    "    validation_y = np.array(y[trainNb:trainNb+validationNb])\n",
    "    test_X = np.array(X[-testNb:]).reshape([-1, size, size, 1])\n",
    "    test_y = np.array(y[-testNb:])\n",
    "    print(\"    Dataset created!\")\n",
    "        \n",
    "    #Save\n",
    "#     saveDataset(train_X, train_y, validation_X, validation_y, test_X, test_y, nbPerGenre, genres, sliceSize)\n",
    "\n",
    "    return train_X, train_y, validation_X, validation_y, test_X, test_y\n",
    "\n",
    "#Saves dataset\n",
    "def saveDataset(train_X, train_y, validation_X, validation_y, test_X, test_y):\n",
    "#      #Create path for dataset if not existing\n",
    "#     if not os.path.exists(os.path.dirname(datasetPath)):\n",
    "#         try:\n",
    "#             os.makedirs(os.path.dirname(datasetPath))\n",
    "#         except OSError as exc: # Guard against race condition\n",
    "#             if exc.errno != errno.EEXIST:\n",
    "#                 raise\n",
    "\n",
    "    #SaveDataset\n",
    "    print(\"[+] Saving dataset... \")\n",
    "    datasetName = \"main\"\n",
    "    pickle.dump(train_X, open(\"train_X_{}.p\".format(datasetName), \"wb\" ))\n",
    "    pickle.dump(train_y, open(\"train_y_{}.p\".format(datasetName), \"wb\" ))\n",
    "    pickle.dump(validation_X, open(\"validation_X_{}.p\".format(datasetName), \"wb\" ))\n",
    "    pickle.dump(validation_y, open(\"validation_y_{}.p\".format(datasetName), \"wb\" ))\n",
    "    pickle.dump(test_X, open(\"test_X_{}.p\".format(datasetName), \"wb\" ))\n",
    "    pickle.dump(test_y, open(\"test_y_{}.p\".format(datasetName), \"wb\" ))\n",
    "    print(\"    Dataset saved! ✅💾\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size (1299, 2)\n",
      "data size (1299, 2)\n",
      "X size (1299, 128, 128, 1)\n",
      "y size (1299,)\n",
      "389 129 781\n",
      "    Dataset created!\n",
      "(781, 128, 128, 1) (781,) (389, 128, 128, 1) (389,) (129, 128, 128, 1) (129,)\n",
      "[0 1]\n",
      "(781, 128, 128, 1) (781, 1) (389, 128, 128, 1) (389, 1) (129, 128, 128, 1) (129, 1)\n",
      "[+] Saving dataset... \n",
      "    Dataset saved! ✅💾\n"
     ]
    }
   ],
   "source": [
    "train_X, train_y, validation_X, validation_y, test_X, test_y = createDataset(128, 0.3, 0.1, True)\n",
    "print(np.shape(train_X), np.shape(train_y), np.shape(validation_X), np.shape(validation_y), np.shape(test_X), np.shape(test_y))\n",
    "print(np.unique(train_y))\n",
    "\n",
    "\n",
    "# print(train_X)\n",
    "# print(train_y)\n",
    "# print(validation_X) \n",
    "# print(validation_y) \n",
    "# print(test_X) \n",
    "# print(test_y)\n",
    "train_y = np.reshape(train_y, (-1, 1))\n",
    "test_y = np.reshape(test_y, (-1, 1))\n",
    "validation_y = np.reshape(validation_y, (-1, 1))\n",
    "print(np.shape(train_X), np.shape(train_y), np.shape(validation_X), np.shape(validation_y), np.shape(test_X), np.shape(test_y))\n",
    "saveDataset(train_X, train_y, validation_X, validation_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Loading training and validation datasets... \n",
      "    Training and validation datasets loaded! ✅\n",
      "[+] Loading testing dataset... \n",
      "    Testing dataset loaded! ✅\n",
      "(781, 128, 128, 1) (781, 1) (389, 128, 128, 1) (389, 1) (129, 128, 128, 1) (129, 1)\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "def loadDataset():\n",
    "    #Load existing\n",
    "    \n",
    "    datasetName = \"main\"\n",
    "    print(\"[+] Loading training and validation datasets... \")\n",
    "    train_X = pickle.load(open(\"train_X_{}.p\".format(datasetName), \"rb\" ))\n",
    "    train_y = pickle.load(open(\"train_y_{}.p\".format(datasetName), \"rb\" ))\n",
    "    validation_X = pickle.load(open(\"validation_X_{}.p\".format(datasetName), \"rb\" ))\n",
    "    validation_y = pickle.load(open(\"validation_y_{}.p\".format(datasetName), \"rb\" ))\n",
    "    print(\"    Training and validation datasets loaded! ✅\")\n",
    "    print(\"[+] Loading testing dataset... \")\n",
    "    test_X = pickle.load(open(\"test_X_{}.p\".format(datasetName), \"rb\" ))\n",
    "    test_y = pickle.load(open(\"test_y_{}.p\".format(datasetName), \"rb\" ))\n",
    "    print(\"    Testing dataset loaded! ✅\")\n",
    "    return train_X, train_y, validation_X, validation_y, test_X, test_y\n",
    "\n",
    "train_X, train_y, validation_X, validation_y, test_X, test_y = loadDataset()\n",
    "\n",
    "print(np.shape(train_X), np.shape(train_y), np.shape(validation_X), np.shape(validation_y), np.shape(test_X), np.shape(test_y))\n",
    "print(np.unique(train_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(train_y)\n",
    "\n",
    "\n",
    "# r = len(train_y)\n",
    "# y = np.reshape(train_y, (1, r))\n",
    "# print(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tflearn # source ~./bash...\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression\n",
    "\n",
    "def createModel(nbClasses,imageSize):\n",
    "    print(\"[+] Creating model...\")\n",
    "    convnet = input_data(shape=[None, imageSize, imageSize, 1], name='input')\n",
    "\n",
    "#     convnet = conv_2d(convnet, 64, 2, activation='elu', weights_init=\"Xavier\")\n",
    "#     convnet = max_pool_2d(convnet, 2)\n",
    "\n",
    "    convnet = conv_2d(convnet, 128, 4, activation='relu', weights_init=\"Xavier\")\n",
    "    convnet = max_pool_2d(convnet, 4)\n",
    "\n",
    "    convnet = conv_2d(convnet, 256, 2, activation='relu', weights_init=\"Xavier\")\n",
    "    convnet = max_pool_2d(convnet, 2)\n",
    "\n",
    "    convnet = conv_2d(convnet, 512, 2, activation='relu', weights_init=\"Xavier\")\n",
    "    convnet = max_pool_2d(convnet, 2)\n",
    "\n",
    "#     convnet = conv_2d(convnet, 128, 10, activation='elu', weights_init=\"Xavier\")\n",
    "#     convnet = max_pool_2d(convnet, 5)\n",
    "    \n",
    "#     convnet = conv_2d(convnet, 32, 5, activation='elu', weights_init=\"Xavier\")\n",
    "#     convnet = max_pool_2d(convnet, 2)\n",
    "\n",
    "#     convnet = fully_connected(convnet, 128, activation='elu')\n",
    "#     convnet = dropout(convnet, 0.75)\n",
    "    \n",
    "    convnet = fully_connected(convnet, 128, activation='relu')\n",
    "    convnet = dropout(convnet, 0.5)\n",
    "\n",
    "    convnet = fully_connected(convnet, 1, activation='sigmoid')\n",
    "    convnet = regression(convnet, optimizer='rmsprop', loss='binary_crossentropy')\n",
    "#     convnet = regression(convnet, optimizer='adam', learning_rate=0.01,\n",
    "#                  loss='mean_square', name='target')\n",
    "\n",
    "    model = tflearn.DNN(convnet)\n",
    "    print(\"    Model created! ✅\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Creating model...\n",
      "    Model created! ✅\n"
     ]
    }
   ],
   "source": [
    "nbClasses = 2\n",
    "sliceSize = 128\n",
    "#Model parameters\n",
    "batchSize = 128\n",
    "learningRate = 0.001\n",
    "nbEpoch = 20\n",
    "\n",
    "model = createModel(nbClasses, sliceSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 75  | total loss: \u001b[1m\u001b[32m0.61245\u001b[0m\u001b[0m | time: 35.591s\n",
      "\u001b[2K\r",
      "| RMSProp | epoch: 011 | loss: 0.61245 - binary_acc: 0.7228 -- iter: 640/781\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "print(\"[+] Training the model...\")\n",
    "model.fit(train_X, train_y, n_epoch=nbEpoch, batch_size=batchSize, shuffle=True, \n",
    "          validation_set=(validation_X, validation_y), snapshot_step=100, show_metric=True)\n",
    "print(\"    Model trained! ✅\")\n",
    "\n",
    "#Save trained model\n",
    "print(\"[+] Saving the weights...\")\n",
    "model.save('musicDNN.tflearn')\n",
    "print(\"[+] Weights saved! ✅💾\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Loading weights...\n",
      "INFO:tensorflow:Restoring parameters from /Users/sangencre/Desktop/NYU/fall_17/ML/final_project/ML_final_project/src/musicDNN.tflearn\n",
      "    Weights loaded! ✅\n",
      "[+] Test accuracy: 0.813953488372093 \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'denselayer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a7eedb31cc27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[+] Test accuracy: {} \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestAccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdenselayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'denselayer' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#Load weights\n",
    "print(\"[+] Loading weights...\")\n",
    "model.load('musicDNN.tflearn')\n",
    "print(\"    Weights loaded! ✅\")\n",
    "\n",
    "testAccuracy = model.evaluate(test_X, test_y)[0]\n",
    "print(\"[+] Test accuracy: {} \".format(testAccuracy))\n",
    "\n",
    "# w = model.get_weights()\n",
    "# print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 24\n",
      "(128, 128, 1)\n",
      "INFO:tensorflow:Restoring parameters from /Users/sangencre/Desktop/NYU/fall_17/ML/final_project/ML_final_project/src/musicDNN.tflearn\n",
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./audioclip-1512874035918-5282_spec.png\"\n",
    "imgData = getImageData(path, 128, True)\n",
    "one_count = 0\n",
    "zero_count = 0\n",
    "for i in test_y:\n",
    "    if i == 1:\n",
    "        one_count+=1 \n",
    "    else:\n",
    "        zero_count += 1\n",
    "print(one_count,zero_count)\n",
    "\n",
    "X_predict = imgData\n",
    "# X_predict = np.reshape(X_predict, (1 , 128, 128, 1))\n",
    "print(np.shape(X_predict))\n",
    "# print(X_predict)\n",
    "model.load('musicDNN.tflearn')\n",
    "print(test_y)\n",
    "model.predict(test_X)\n",
    "# testAccuracy = model.evaluate(test_X, test_y)[0]\n",
    "# print(\"[+] Test accuracy: {} \".format(testAccuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def cnn_model_fn(features, labels, mode):\n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "    # Input Layer\n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, 128, 128, 1])\n",
    "\n",
    "    # Convolutional Layer #1\n",
    "    conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=128,\n",
    "      kernel_size=[4, 4],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #1\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[4, 4], strides=4)\n",
    "\n",
    "    # Convolutional Layer #2 and Pooling Layer #2\n",
    "    conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=256,\n",
    "      kernel_size=[2, 2],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "        # Convolutional Layer #2 and Pooling Layer #2\n",
    "    conv3 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=512,\n",
    "      kernel_size=[2, 2],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "    pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Dense Layer\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(\n",
    "      inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    # Logits Layer\n",
    "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "\n",
    "    predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)\n",
    "    loss = tf.losses.softmax_cross_entropy(\n",
    "      onehot_labels=onehot_labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    train_op = optimizer.minimize(\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pocket Dial Classification Deep Neural Network\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import torchvision.utils as vutils\n",
    "import math\n",
    "import torch.nn.functional as Funct\n",
    "\n",
    "# Down-sampling block in the Unet Architecture \n",
    "class conv_block(nn.Module):\n",
    "    #using the input channels I specify the channels at for repeated use of this block\n",
    "    def __init__(self):\n",
    "        super(down_block,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3 , 64, kernel_size=(2,2),stride=1,padding=0,dilation=0, bias=True)\n",
    "        self.elu1 = nn.ELU(inplace=True)\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2,2), stride=1, return_indices = False)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(64, 128,kernel_size=(2,2),stride=1,padding=0,dilation=0, bias=True)\n",
    "        self.elu2 = nn.ELU(inplace=True)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2,2), stride=1, return_indices = False)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(128 , 256,kernel_size=(2,2),stride=1,padding=0,dilation=0, bias=True)\n",
    "        self.elu3 = nn.ELU(inplace=True)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=(2,2), stride=1, return_indices = False)\n",
    "\n",
    "        self.conv4 = nn.Conv2d( 256, 512,kernel_size=(2,2),stride=1,padding=0,dilation=0 ,bias=True)\n",
    "        self.elu4 = nn.ELU(inplace=True)\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=(2,2), stride=1, return_indices = False)\n",
    "\n",
    "        self.fully_connected = nn.Linear(in_features, out_features, bias=True)\n",
    "        self.drop = nn.Dropout(p=0.5, inplace=True)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_uniform(m.weight)\n",
    "\n",
    "\n",
    "    #forward function through the block\n",
    "    def forward(self, x):\n",
    "        fwd_map = self.conv1(x)\n",
    "        fwd_map = self.elu1(fwd_map)\n",
    "        fwd_map = self.maxpool1(fwd_map)\n",
    "\n",
    "        fwd_map = self.conv2(x)\n",
    "        fwd_map = self.elu2(fwd_map)\n",
    "        fwd_map = self.maxpool2(fwd_map)\n",
    "\n",
    "        fwd_map = self.conv3(x)\n",
    "        fwd_map = self.elu3(fwd_map)\n",
    "        fwd_map = self.maxpool3(fwd_map)\n",
    "\n",
    "        fwd_map = self.conv4(x)\n",
    "        fwd_map = self.elu4(fwd_map)\n",
    "        fwd_map = self.maxpool4(fwd_map)\n",
    "\n",
    "        return (fwd_map)\n",
    "\n",
    "\n",
    "#Combining both the down-sampling and up-sampling block into one network.\n",
    "class network(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(network,self).__init__()\n",
    "        self.convolutions = conv_block()\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        fw_out = self.convolutions(x)\n",
    "        out = Funct.softmax(fw_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np \n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from model import network\n",
    "from torchvision import datasets,models,transforms\n",
    "from data_loader import data_loader_seg\n",
    "import torch.optim as optim\n",
    "\n",
    "model_ft = network()\n",
    "\n",
    "\n",
    "# trans = transforms.Compose([transforms.Scale(700), \n",
    "#     transforms.CenterCrop(572),\n",
    "#     transforms.ToTensor()])\n",
    "\n",
    "# dsets = data_loader_seg('/Users/devansh20la/Desktop/data_road/training/',trans = trans)\n",
    "# dsets_enqueuer = torch.utils.data.DataLoader(dsets, batch_size=1, num_workers=0, drop_last=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RMSprop(model_ft.parameters(), lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     criterion = criterion.cuda()\n",
    "\n",
    "loss_data = 0.0\n",
    "\n",
    "for Epoch in range(10):\n",
    "\n",
    "    for idx,data in enumerate(dsets_enqueuer,1):\n",
    "\n",
    "        image,image_seg = data['image'], data['image_seg']\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            image, image_seg = Variable(image.cuda(), requires_grad = False), Variable(image_seg.cuda(), requires_grad = False)\n",
    "        else:\n",
    "            image, image_seg = Variable(image, requires_grad = False), Variable(image_seg, requires_grad = False)\n",
    "\n",
    "        output = model_ft(image)\n",
    "        loss = criterion(output,image_seg)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_data += loss.data\n",
    "        print (\"Epoch {0} /10, loss = {1}\".format(Epoch,loss_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
